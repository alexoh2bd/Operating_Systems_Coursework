note: I reformatted the vmstat columns after writing the sample-output to be easier to read.
I create a c program that writes about 3 GB to RAM, RAM is 2 GB. In the sample output, I are looking at the result of procstat and vmstat. I run vmstat in a separate process swap-this-vm documenting the vm process statistics once every second while running the swap-this process. 

In vmstat, I see that before running swap-this, there are about 545 megabytes of mapped virtual memory, 1.5 GB of free RAM, 2 swapped out threads, and not much page, disk, page fault, or cpu activity. Many of the other statistics show 0 and appear to be typical when compared to the next few seconds.

When I allocate the new memory, I see a change in many of these statistics over the next 7 seconds. I now map around 4.2 GB, leaving only 7-31 total mb. The number of page faults spikes to 386204 per second and continues at a rate of 36127-70922 per second over the next 6 seconds. The number of pages paged-out increases from 0 to 29 to 1000-2000, meaning that many more pages are getting moved to disk. The swapped pages are shown by the number of freed pages, which over this period ranges from 1390-70922. The kernel has run out of disc, and tries to free the least recently used pages in physical RAM in order to efficiently swap them to disc. Other noticeable consequences of the mass mapping are the increase are the pages scanned, the disk operations per second, the number of device interrupts, and user time for normal and low priority processes,and the system and interrupt time. 

Somewhere during the 8th second, the memory is all freed, and the mapped memory and freed memory revert to numbers comparable to before the allocation. 3970202 pages are freed, and 56 pages are paged in. I think this means that the pages which were in RAM before, the 57 pages which were reactivated at the start of the allocation, were brought back from disk. Afterwards, the statistics reach numbers similar to before the allocation.


I ran this process separately in dtrace to track the time spent on each page fault. Many page faults were called and at the top they were mostly within the 1000-4000 nanosecond range. As I scrolled down, there were frequent spurts of page faults which ranged from 150000-500000 ns. It is presumed that these page faults dealt with page swaps to disk because disk takes longer to communicate with than physical RAM.
